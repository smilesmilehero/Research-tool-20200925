{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as pltimport \n",
    "%matplotlib inline\n",
    "from timeit import default_timer as timer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Making CSV Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(csvName, dataDir):\n",
    "    with open(csvName,\"w\") as file:\n",
    "        file.write(\"ID,Class\\n\")\n",
    "        for fn in dataDir:\n",
    "            fileID  = fn.split('-')[-1].split('')[0]\n",
    "            classID = fileID.split('.')[-1]\n",
    "            file.write(str(fileID)+','+classID)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Making .csv files #############\n",
    "\n",
    "#train_dir = np.array(glob(\"train/*\"))\n",
    "#write_to_csv(\"train.csv\",train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Making .csv files #############\n",
    "\n",
    "#test_dir = np.array(glob(\"test/*\"))\n",
    "#write_to_csv(\"test.csv\",test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Feature extraction & Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:  3787.8247444\n",
      "end:  3787.8313901\n",
      "time taken: 0.0066457000002628774 minutes 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Testing the extraction of MFCC & others using Librosa\n",
    "filename = 'train/1-3.wav'\n",
    "X, sample_rate = librosa.load(filename) \n",
    "start = timer()#####TONY\n",
    "mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "end = timer()#######TONY\n",
    "print(\"start: \", start)\n",
    "print(\"end: \", end)\n",
    "print(\"time taken: {0} minutes {1:.1f} seconds\".format((end - start), (end - start)%60))#######TONY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature extraction & Spec Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting MFCC\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "    \n",
    "   \n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) #40######\n",
    "        mfcc_delta = librosa.feature.delta(mfccs) #TONY\n",
    "        mfcc_delta2 = librosa.feature.delta(mfccs, order=2)#TONY\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "        ###### ADD NEW FEATURES (SPECTRAL RELATED)##### 24-SEP\n",
    "        cent = np.mean(librosa.feature.spectral_centroid(y=X, sr=sample_rate).T,axis=0)\n",
    "        flatness = np.mean(librosa.feature.spectral_flatness(y=X).T,axis=0)\n",
    "        rolloff = np.mean(librosa.feature.spectral_rolloff(S=stft, sr=sample_rate).T,axis=0)\n",
    "        rms = np.mean(librosa.feature.rms(S=stft).T,axis=0)\n",
    "        ext_features = np.hstack([mfccs,mfcc_delta, mfcc_delta2, chroma, mel, contrast, tonnetz, cent,flatness, rolloff,rms])\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return np.array(ext_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:  3787.8247444\n",
      "end:  3826.8776972\n",
      "time taken: 39.052952800000185 minutes 39.1 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(277,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testFile='train/1-3.wav'\n",
    "testF = extract_features(testFile)\n",
    "end = timer()#######TONY\n",
    "print(\"start: \", start)\n",
    "print(\"end: \", end)\n",
    "print(\"time taken: {0} minutes {1:.1f} seconds\".format((end - start), (end - start)%60))#######TONY\n",
    "testF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Spectrogram Features\n",
    "def extract_features_spec(file_name):\n",
    "   \n",
    "    try:\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        stft = np.abs(librosa.stft(X))\n",
    "        \n",
    "        mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "        ext_features = mel\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return np.array(ext_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testFile = 'ALL_A_TRAIN/01-01.wav'\n",
    "testF2 = extract_features_spec(testFile)\n",
    "testF2.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "def create_mlp():    \n",
    "    # Construct model \n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(277,)))    ### 256\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    return model\n",
    "def create_cnn(width, height, depth, filters=(16, 32, 64)):\n",
    "    # First CONV layer\n",
    "    inputShape = (height, width, depth)\n",
    "    inputs = Input(shape=inputShape)\n",
    "    x = inputs\n",
    "    x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # 2nd CONV\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)    \n",
    "    # 3rd CONV\n",
    "    x = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # 4th CONV\n",
    "    x = Conv2D(64, (3, 3))(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.5)(x)    \n",
    "    # 5th CONV\n",
    "    x = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)   \n",
    "    # 6th CONV\n",
    "    x = Conv2D(128, (3, 3))(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.7)(x)                             #0.5\n",
    "    # Flatten and Output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dropout(0.7)(x)                             #0.5\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)  \n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Audio Features: Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv('train.csv',dtype=str)\n",
    "trainAudioPath = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting Features...\n",
      "Finished feature extraction from  560 files\n"
     ]
    }
   ],
   "source": [
    "# MFCCs Features\n",
    "# Metadata file & full UrbanSound dataset \n",
    "train_features = []\n",
    "print(\"[INFO] Extracting Features...\")\n",
    "for fileID in traindf[\"ID\"]:\n",
    "    audioFile = fileID + '.wav'\n",
    "    audioDir = trainAudioPath + audioFile\n",
    "    data = extract_features(audioDir)\n",
    "    features.append(data)  \n",
    "print('Finished feature extraction from ', len(features), 'files') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array is saved in the .npy file\n"
     ]
    }
   ],
   "source": [
    "np.save('trainAudioFeature277-111', features) \n",
    "print(\"The array is saved in the .npy file\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auido Features: Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.read_csv('test.csv', dtype=str)\n",
    "testAudioPath = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting Features...\n",
      "Finished feature extraction from  120 files\n"
     ]
    }
   ],
   "source": [
    "# MFCCs Features\n",
    "# Metadata file & full UrbanSound dataset \n",
    "\n",
    "test_features = []\n",
    "print(\"[INFO] Extracting Features...\")\n",
    "for fileID in testdf[\"ID\"]:\n",
    "    audioFile = fileID + '.wav'\n",
    "    audioDir = testAudioPath + audioFile\n",
    "    #print(audioPath)\n",
    "    data = extract_features(audioDir)\n",
    "    test_features.append(data)\n",
    "    \n",
    "print('Finished feature extraction from ', len(test_features), 'files') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array is saved in the .npy file\n"
     ]
    }
   ],
   "source": [
    "# Save to binary file\n",
    "\n",
    "np.save('testAudioFeature277-222', test_features) \n",
    "print(\"The array is saved in the .npy file\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Audio Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAudioFeatures = np.load('trainAudioFeature277.npy', allow_pickle=True)\n",
    "testAudioFeatures = np.load('testAudioFeature277.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAudioFeatures = np.array(trainAudioFeatures)\n",
    "testAudioFeatures = np.array(testAudioFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk \n",
    "scaler1 = sk.preprocessing.StandardScaler().fit(trainAudioFeatures)\n",
    "trainAudioFeatures = scaler1.transform(trainAudioFeatures)\n",
    "testAudioFeatures = scaler1.transform(testAudioFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils.np_utils import to_categorical\n",
    "# LABELS\n",
    "train_labels = traindf[\"Class\"]\n",
    "test_labels = testdf[\"Class\"]\n",
    "#labels = to_categorical(labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 11,  2, ..., 11, 11,  6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training MLP (1-D Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading data...\n",
      "[INFO] processing data...\n",
      "WARNING:tensorflow:From C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import os\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "print(\"[INFO] loading data...\")\n",
    "#features, images, labels = features, images, labels\n",
    "\n",
    "print(\"[INFO] processing data...\")\n",
    "#split = train_test_split(features, images, labels, test_size=0.25, random_state=42)\n",
    "#(trainFeatureX, testFeatureX, trainImagesX, testImagesX, trainY, testY) = split\n",
    "\n",
    "\n",
    "trainFeatureX, testFeatureX, trainY, testY = trainAudioFeatures, testAudioFeatures, train_labels, test_labels\n",
    "\n",
    "# For prediction and Confusion Matrix\n",
    "trueY = testY\n",
    "\n",
    "trainY = to_categorical(trainY, 16)\n",
    "testY = to_categorical(testY, 16)\n",
    "\n",
    "\n",
    "# create the MLP and CNN models\n",
    "mlp = create_mlp()\n",
    "#cnn = create_cnn(64, 64, 3)\n",
    "\n",
    "# create the input to our final set of layers as the *output* of both the MLP and CNN\n",
    "#combinedInput = concatenate([mlp.output, cnn.output])\n",
    "\n",
    "Input = mlp.output\n",
    "\n",
    "# The final FC layer head will have two dense layers, the final one being softmax layer\n",
    "x = Dense(512, activation=\"relu\")(Input)\n",
    "x = Dense(16, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model will accept audio on the MLP input and SPEC on the CNN input, outputting prediction\n",
    "model = Model(inputs=mlp.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2112, 277)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeatureX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.06872478,  1.28338691, -0.4936356 , ..., -0.60741306,\n",
       "        -0.06829435,  1.05904598],\n",
       "       [ 1.15407798,  1.35957986, -0.83394889, ..., -0.61292337,\n",
       "        -0.19388473,  1.3999928 ],\n",
       "       [ 0.33347062,  0.06654313,  0.32404226, ...,  1.63873313,\n",
       "        -0.74975233, -0.056732  ],\n",
       "       ...,\n",
       "       [-0.94423966, -0.41541648,  1.14189764, ..., -0.08278825,\n",
       "        -0.88765522, -1.08357268],\n",
       "       [ 0.68143011, -0.09756506, -0.82823552, ...,  0.34309194,\n",
       "        -0.2420719 ,  0.9511039 ],\n",
       "       [-1.92661784, -1.01802822,  0.69303722, ..., -0.60270901,\n",
       "         0.19906007, -1.27859838]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFeatureX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2885: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2889: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "[INFO] training model...\n",
      "WARNING:tensorflow:From C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "Train on 2112 samples, validate on 960 samples\n",
      "Epoch 1/250\n",
      "2112/2112 [==============================] - 3s 1ms/step - loss: 2.2510 - acc: 0.2784 - val_loss: 1.4919 - val_acc: 0.6260\n",
      "Epoch 2/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 1.5423 - acc: 0.4806 - val_loss: 1.0184 - val_acc: 0.6687\n",
      "Epoch 3/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 1.2327 - acc: 0.6032 - val_loss: 0.7146 - val_acc: 0.7760\n",
      "Epoch 4/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 1.0292 - acc: 0.6619 - val_loss: 0.5681 - val_acc: 0.8271\n",
      "Epoch 5/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.9388 - acc: 0.6984 - val_loss: 0.4854 - val_acc: 0.8448\n",
      "Epoch 6/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.8264 - acc: 0.7173 - val_loss: 0.4609 - val_acc: 0.8469\n",
      "Epoch 7/250\n",
      "2112/2112 [==============================] - 0s 103us/step - loss: 0.7640 - acc: 0.7557 - val_loss: 0.4046 - val_acc: 0.8656\n",
      "Epoch 8/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.6950 - acc: 0.7713 - val_loss: 0.3653 - val_acc: 0.8865\n",
      "Epoch 9/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.6362 - acc: 0.7846 - val_loss: 0.3210 - val_acc: 0.9031\n",
      "Epoch 10/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.5985 - acc: 0.8078 - val_loss: 0.2988 - val_acc: 0.9073\n",
      "Epoch 11/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.5486 - acc: 0.8220 - val_loss: 0.2799 - val_acc: 0.9083\n",
      "Epoch 12/250\n",
      "2112/2112 [==============================] - 0s 102us/step - loss: 0.5525 - acc: 0.8182 - val_loss: 0.2476 - val_acc: 0.9208\n",
      "Epoch 13/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.4849 - acc: 0.8381 - val_loss: 0.2412 - val_acc: 0.9281\n",
      "Epoch 14/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.4842 - acc: 0.8381 - val_loss: 0.2195 - val_acc: 0.9323\n",
      "Epoch 15/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.4465 - acc: 0.8532 - val_loss: 0.2442 - val_acc: 0.9292\n",
      "Epoch 16/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.4303 - acc: 0.8627 - val_loss: 0.2188 - val_acc: 0.9229\n",
      "Epoch 17/250\n",
      "2112/2112 [==============================] - 0s 101us/step - loss: 0.3911 - acc: 0.8778 - val_loss: 0.2106 - val_acc: 0.9354\n",
      "Epoch 18/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.3954 - acc: 0.8736 - val_loss: 0.1972 - val_acc: 0.9385\n",
      "Epoch 19/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.3672 - acc: 0.8840 - val_loss: 0.2075 - val_acc: 0.9427\n",
      "Epoch 20/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.3864 - acc: 0.8797 - val_loss: 0.1921 - val_acc: 0.9417\n",
      "Epoch 21/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.3811 - acc: 0.8849 - val_loss: 0.1888 - val_acc: 0.9385\n",
      "Epoch 22/250\n",
      "2112/2112 [==============================] - 0s 102us/step - loss: 0.3493 - acc: 0.8930 - val_loss: 0.1829 - val_acc: 0.9458\n",
      "Epoch 23/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.3412 - acc: 0.8982 - val_loss: 0.1672 - val_acc: 0.9469\n",
      "Epoch 24/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.3330 - acc: 0.8987 - val_loss: 0.1721 - val_acc: 0.9448\n",
      "Epoch 25/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.3073 - acc: 0.9039 - val_loss: 0.1678 - val_acc: 0.9510\n",
      "Epoch 26/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.2962 - acc: 0.9062 - val_loss: 0.1853 - val_acc: 0.9510\n",
      "Epoch 27/250\n",
      "2112/2112 [==============================] - 0s 102us/step - loss: 0.3059 - acc: 0.9081 - val_loss: 0.1919 - val_acc: 0.9542\n",
      "Epoch 28/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.3069 - acc: 0.8987 - val_loss: 0.1958 - val_acc: 0.9510\n",
      "Epoch 29/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.3009 - acc: 0.9072 - val_loss: 0.1828 - val_acc: 0.9490\n",
      "Epoch 30/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.2677 - acc: 0.9162 - val_loss: 0.1686 - val_acc: 0.9542\n",
      "Epoch 31/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.2895 - acc: 0.9181 - val_loss: 0.1537 - val_acc: 0.9635\n",
      "Epoch 32/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.2785 - acc: 0.9186 - val_loss: 0.1666 - val_acc: 0.9552\n",
      "Epoch 33/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.2808 - acc: 0.9152 - val_loss: 0.1651 - val_acc: 0.9615\n",
      "Epoch 34/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.2550 - acc: 0.9214 - val_loss: 0.1657 - val_acc: 0.9604\n",
      "Epoch 35/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.2578 - acc: 0.9238 - val_loss: 0.1526 - val_acc: 0.9583\n",
      "Epoch 36/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.2695 - acc: 0.9171 - val_loss: 0.1789 - val_acc: 0.9542\n",
      "Epoch 37/250\n",
      "2112/2112 [==============================] - 0s 101us/step - loss: 0.2441 - acc: 0.9257 - val_loss: 0.1548 - val_acc: 0.9625\n",
      "Epoch 38/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.2386 - acc: 0.9318 - val_loss: 0.1665 - val_acc: 0.9563\n",
      "Epoch 39/250\n",
      "2112/2112 [==============================] - 0s 91us/step - loss: 0.2363 - acc: 0.9342 - val_loss: 0.1803 - val_acc: 0.9594\n",
      "Epoch 40/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.2355 - acc: 0.9342 - val_loss: 0.1603 - val_acc: 0.9594\n",
      "Epoch 41/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.2673 - acc: 0.9252 - val_loss: 0.1806 - val_acc: 0.9594\n",
      "Epoch 42/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.2414 - acc: 0.9290 - val_loss: 0.1684 - val_acc: 0.9625\n",
      "Epoch 43/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.2283 - acc: 0.9323 - val_loss: 0.1343 - val_acc: 0.9719\n",
      "Epoch 44/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.2275 - acc: 0.9361 - val_loss: 0.1623 - val_acc: 0.9667\n",
      "Epoch 45/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.2153 - acc: 0.9394 - val_loss: 0.1385 - val_acc: 0.9688\n",
      "Epoch 46/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.2202 - acc: 0.9384 - val_loss: 0.1598 - val_acc: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.2026 - acc: 0.9465 - val_loss: 0.1503 - val_acc: 0.9656\n",
      "Epoch 48/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1929 - acc: 0.9474 - val_loss: 0.1679 - val_acc: 0.9698\n",
      "Epoch 49/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.2424 - acc: 0.9399 - val_loss: 0.1455 - val_acc: 0.9677\n",
      "Epoch 50/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.2337 - acc: 0.9356 - val_loss: 0.1542 - val_acc: 0.9688\n",
      "Epoch 51/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.2032 - acc: 0.9418 - val_loss: 0.1489 - val_acc: 0.9677\n",
      "Epoch 52/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.2029 - acc: 0.9441 - val_loss: 0.1583 - val_acc: 0.9688\n",
      "Epoch 53/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1790 - acc: 0.9460 - val_loss: 0.1374 - val_acc: 0.9698\n",
      "Epoch 54/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.2614 - acc: 0.9351 - val_loss: 0.1523 - val_acc: 0.9656\n",
      "Epoch 55/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.2303 - acc: 0.9384 - val_loss: 0.1561 - val_acc: 0.9729\n",
      "Epoch 56/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.2371 - acc: 0.9489 - val_loss: 0.1676 - val_acc: 0.9688\n",
      "Epoch 57/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.2008 - acc: 0.9470 - val_loss: 0.1542 - val_acc: 0.9771\n",
      "Epoch 58/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.2056 - acc: 0.9498 - val_loss: 0.1479 - val_acc: 0.9708\n",
      "Epoch 59/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1825 - acc: 0.9484 - val_loss: 0.1459 - val_acc: 0.9792\n",
      "Epoch 60/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1690 - acc: 0.9536 - val_loss: 0.1460 - val_acc: 0.9729\n",
      "Epoch 61/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.2138 - acc: 0.9489 - val_loss: 0.1426 - val_acc: 0.9729\n",
      "Epoch 62/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.1724 - acc: 0.9555 - val_loss: 0.1637 - val_acc: 0.9740\n",
      "Epoch 63/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1759 - acc: 0.9593 - val_loss: 0.1444 - val_acc: 0.9667\n",
      "Epoch 64/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.2263 - acc: 0.9484 - val_loss: 0.1494 - val_acc: 0.9708\n",
      "Epoch 65/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.2154 - acc: 0.9460 - val_loss: 0.1371 - val_acc: 0.9750\n",
      "Epoch 66/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1768 - acc: 0.9612 - val_loss: 0.1713 - val_acc: 0.9646\n",
      "Epoch 67/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.2059 - acc: 0.9479 - val_loss: 0.1501 - val_acc: 0.9698\n",
      "Epoch 68/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1840 - acc: 0.9555 - val_loss: 0.1562 - val_acc: 0.9729\n",
      "Epoch 69/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1899 - acc: 0.9498 - val_loss: 0.1456 - val_acc: 0.9771\n",
      "Epoch 70/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1768 - acc: 0.9560 - val_loss: 0.1379 - val_acc: 0.9781\n",
      "Epoch 71/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1893 - acc: 0.9536 - val_loss: 0.1527 - val_acc: 0.9719\n",
      "Epoch 72/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1966 - acc: 0.9503 - val_loss: 0.1758 - val_acc: 0.9656\n",
      "Epoch 73/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1629 - acc: 0.9574 - val_loss: 0.1536 - val_acc: 0.9688\n",
      "Epoch 74/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1860 - acc: 0.9555 - val_loss: 0.1538 - val_acc: 0.9760\n",
      "Epoch 75/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.1506 - acc: 0.9640 - val_loss: 0.1910 - val_acc: 0.9688\n",
      "Epoch 76/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1885 - acc: 0.9512 - val_loss: 0.1504 - val_acc: 0.9719\n",
      "Epoch 77/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1903 - acc: 0.9531 - val_loss: 0.1798 - val_acc: 0.9698\n",
      "Epoch 78/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1798 - acc: 0.9593 - val_loss: 0.1805 - val_acc: 0.9708\n",
      "Epoch 79/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1715 - acc: 0.9626 - val_loss: 0.1504 - val_acc: 0.9719\n",
      "Epoch 80/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1863 - acc: 0.9545 - val_loss: 0.1509 - val_acc: 0.9750\n",
      "Epoch 81/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.1540 - acc: 0.9631 - val_loss: 0.1677 - val_acc: 0.9698\n",
      "Epoch 82/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1709 - acc: 0.9607 - val_loss: 0.1673 - val_acc: 0.9719\n",
      "Epoch 83/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.2118 - acc: 0.9545 - val_loss: 0.1504 - val_acc: 0.9740\n",
      "Epoch 84/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.2093 - acc: 0.9550 - val_loss: 0.1537 - val_acc: 0.9760\n",
      "Epoch 85/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1787 - acc: 0.9593 - val_loss: 0.1508 - val_acc: 0.9729\n",
      "Epoch 86/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1542 - acc: 0.9635 - val_loss: 0.1283 - val_acc: 0.9771\n",
      "Epoch 87/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1803 - acc: 0.9602 - val_loss: 0.1628 - val_acc: 0.9708\n",
      "Epoch 88/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1853 - acc: 0.9541 - val_loss: 0.1637 - val_acc: 0.9719\n",
      "Epoch 89/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1671 - acc: 0.9598 - val_loss: 0.1534 - val_acc: 0.9719\n",
      "Epoch 90/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1769 - acc: 0.9588 - val_loss: 0.1412 - val_acc: 0.9719\n",
      "Epoch 91/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1683 - acc: 0.9569 - val_loss: 0.1416 - val_acc: 0.9667\n",
      "Epoch 92/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.2010 - acc: 0.9550 - val_loss: 0.1398 - val_acc: 0.9719\n",
      "Epoch 93/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.1556 - acc: 0.9588 - val_loss: 0.1432 - val_acc: 0.9698\n",
      "Epoch 94/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1943 - acc: 0.9574 - val_loss: 0.1407 - val_acc: 0.9781\n",
      "Epoch 95/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1763 - acc: 0.9612 - val_loss: 0.1381 - val_acc: 0.9781\n",
      "Epoch 96/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1842 - acc: 0.9598 - val_loss: 0.1738 - val_acc: 0.9646\n",
      "Epoch 97/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.1605 - acc: 0.9669 - val_loss: 0.1447 - val_acc: 0.9729\n",
      "Epoch 98/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.1698 - acc: 0.9631 - val_loss: 0.1533 - val_acc: 0.9719\n",
      "Epoch 99/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1561 - acc: 0.9635 - val_loss: 0.1561 - val_acc: 0.9719\n",
      "Epoch 100/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1758 - acc: 0.9616 - val_loss: 0.1336 - val_acc: 0.9740\n",
      "Epoch 101/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1807 - acc: 0.9640 - val_loss: 0.1486 - val_acc: 0.9740\n",
      "Epoch 102/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.1540 - acc: 0.9650 - val_loss: 0.1228 - val_acc: 0.9781\n",
      "Epoch 103/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1897 - acc: 0.9569 - val_loss: 0.1333 - val_acc: 0.9802\n",
      "Epoch 104/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1489 - acc: 0.9683 - val_loss: 0.1676 - val_acc: 0.9729\n",
      "Epoch 105/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1544 - acc: 0.9631 - val_loss: 0.1390 - val_acc: 0.9781\n",
      "Epoch 106/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1612 - acc: 0.9602 - val_loss: 0.1615 - val_acc: 0.9677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/250\n",
      "2112/2112 [==============================] - 0s 91us/step - loss: 0.1442 - acc: 0.9626 - val_loss: 0.1474 - val_acc: 0.9740\n",
      "Epoch 108/250\n",
      "2112/2112 [==============================] - 0s 91us/step - loss: 0.1456 - acc: 0.9598 - val_loss: 0.1456 - val_acc: 0.9802\n",
      "Epoch 109/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1553 - acc: 0.9683 - val_loss: 0.1325 - val_acc: 0.9802\n",
      "Epoch 110/250\n",
      "2112/2112 [==============================] - 0s 101us/step - loss: 0.1376 - acc: 0.9654 - val_loss: 0.1283 - val_acc: 0.9802\n",
      "Epoch 111/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1472 - acc: 0.9616 - val_loss: 0.1460 - val_acc: 0.9688\n",
      "Epoch 112/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1565 - acc: 0.9626 - val_loss: 0.1657 - val_acc: 0.9729\n",
      "Epoch 113/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1245 - acc: 0.9735 - val_loss: 0.1582 - val_acc: 0.9729\n",
      "Epoch 114/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1659 - acc: 0.9545 - val_loss: 0.1385 - val_acc: 0.9750\n",
      "Epoch 115/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.1431 - acc: 0.9659 - val_loss: 0.1447 - val_acc: 0.9750\n",
      "Epoch 116/250\n",
      "2112/2112 [==============================] - 0s 91us/step - loss: 0.2147 - acc: 0.9560 - val_loss: 0.1414 - val_acc: 0.9750\n",
      "Epoch 117/250\n",
      "2112/2112 [==============================] - 0s 91us/step - loss: 0.1604 - acc: 0.9645 - val_loss: 0.1437 - val_acc: 0.9750\n",
      "Epoch 118/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1786 - acc: 0.9692 - val_loss: 0.1451 - val_acc: 0.9729\n",
      "Epoch 119/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1750 - acc: 0.9635 - val_loss: 0.1635 - val_acc: 0.9740\n",
      "Epoch 120/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1564 - acc: 0.9673 - val_loss: 0.1475 - val_acc: 0.9750\n",
      "Epoch 121/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1793 - acc: 0.9612 - val_loss: 0.1536 - val_acc: 0.9771\n",
      "Epoch 122/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1764 - acc: 0.9635 - val_loss: 0.1567 - val_acc: 0.9750\n",
      "Epoch 123/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1379 - acc: 0.9678 - val_loss: 0.1635 - val_acc: 0.9740\n",
      "Epoch 124/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1482 - acc: 0.9626 - val_loss: 0.1621 - val_acc: 0.9750\n",
      "Epoch 125/250\n",
      "2112/2112 [==============================] - 0s 91us/step - loss: 0.1529 - acc: 0.9669 - val_loss: 0.1407 - val_acc: 0.9771\n",
      "Epoch 126/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1376 - acc: 0.9664 - val_loss: 0.1602 - val_acc: 0.9750\n",
      "Epoch 127/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1594 - acc: 0.9659 - val_loss: 0.1508 - val_acc: 0.9792\n",
      "Epoch 128/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1619 - acc: 0.9640 - val_loss: 0.1621 - val_acc: 0.9750\n",
      "Epoch 129/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1415 - acc: 0.9626 - val_loss: 0.1724 - val_acc: 0.9708\n",
      "Epoch 130/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1882 - acc: 0.9645 - val_loss: 0.1646 - val_acc: 0.9698\n",
      "Epoch 131/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1643 - acc: 0.9697 - val_loss: 0.1618 - val_acc: 0.9760\n",
      "Epoch 132/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1558 - acc: 0.9669 - val_loss: 0.1371 - val_acc: 0.9771\n",
      "Epoch 133/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1497 - acc: 0.9706 - val_loss: 0.1605 - val_acc: 0.9740\n",
      "Epoch 134/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1411 - acc: 0.9659 - val_loss: 0.1468 - val_acc: 0.9771\n",
      "Epoch 135/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1366 - acc: 0.9706 - val_loss: 0.1338 - val_acc: 0.9812\n",
      "Epoch 136/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.1410 - acc: 0.9721 - val_loss: 0.1547 - val_acc: 0.9760\n",
      "Epoch 137/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1553 - acc: 0.9645 - val_loss: 0.1460 - val_acc: 0.9771\n",
      "Epoch 138/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1336 - acc: 0.9721 - val_loss: 0.1480 - val_acc: 0.9708\n",
      "Epoch 139/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1581 - acc: 0.9673 - val_loss: 0.1424 - val_acc: 0.9760\n",
      "Epoch 140/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1284 - acc: 0.9692 - val_loss: 0.1430 - val_acc: 0.9760\n",
      "Epoch 141/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1539 - acc: 0.9678 - val_loss: 0.1413 - val_acc: 0.9792\n",
      "Epoch 142/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1439 - acc: 0.9697 - val_loss: 0.1500 - val_acc: 0.9781\n",
      "Epoch 143/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1513 - acc: 0.9702 - val_loss: 0.1812 - val_acc: 0.9677\n",
      "Epoch 144/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1353 - acc: 0.9706 - val_loss: 0.1553 - val_acc: 0.9781\n",
      "Epoch 145/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1226 - acc: 0.9706 - val_loss: 0.1491 - val_acc: 0.9750\n",
      "Epoch 146/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1383 - acc: 0.9706 - val_loss: 0.1786 - val_acc: 0.9781\n",
      "Epoch 147/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.2231 - acc: 0.9645 - val_loss: 0.1528 - val_acc: 0.9698\n",
      "Epoch 148/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1851 - acc: 0.9673 - val_loss: 0.1575 - val_acc: 0.9740\n",
      "Epoch 149/250\n",
      "2112/2112 [==============================] - 0s 101us/step - loss: 0.1368 - acc: 0.9706 - val_loss: 0.1625 - val_acc: 0.9740\n",
      "Epoch 150/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1175 - acc: 0.9749 - val_loss: 0.1547 - val_acc: 0.9781\n",
      "Epoch 151/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1487 - acc: 0.9645 - val_loss: 0.1595 - val_acc: 0.9771\n",
      "Epoch 152/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1387 - acc: 0.9702 - val_loss: 0.1487 - val_acc: 0.9781\n",
      "Epoch 153/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1482 - acc: 0.9725 - val_loss: 0.1504 - val_acc: 0.9729\n",
      "Epoch 154/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1183 - acc: 0.9725 - val_loss: 0.1433 - val_acc: 0.9760\n",
      "Epoch 155/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1503 - acc: 0.9711 - val_loss: 0.1542 - val_acc: 0.9729\n",
      "Epoch 156/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1555 - acc: 0.9688 - val_loss: 0.1621 - val_acc: 0.9740\n",
      "Epoch 157/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1528 - acc: 0.9702 - val_loss: 0.1714 - val_acc: 0.9750\n",
      "Epoch 158/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1609 - acc: 0.9650 - val_loss: 0.1525 - val_acc: 0.9781\n",
      "Epoch 159/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1346 - acc: 0.9702 - val_loss: 0.1399 - val_acc: 0.9802\n",
      "Epoch 160/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1387 - acc: 0.9725 - val_loss: 0.1536 - val_acc: 0.9802\n",
      "Epoch 161/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1363 - acc: 0.9744 - val_loss: 0.1635 - val_acc: 0.9729\n",
      "Epoch 162/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.1705 - acc: 0.9692 - val_loss: 0.1620 - val_acc: 0.9760\n",
      "Epoch 163/250\n",
      "2112/2112 [==============================] - 0s 91us/step - loss: 0.1290 - acc: 0.9735 - val_loss: 0.1597 - val_acc: 0.9760\n",
      "Epoch 164/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.0994 - acc: 0.9768 - val_loss: 0.1561 - val_acc: 0.9792\n",
      "Epoch 165/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1215 - acc: 0.9744 - val_loss: 0.1656 - val_acc: 0.9760\n",
      "Epoch 166/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.1401 - acc: 0.9754 - val_loss: 0.1601 - val_acc: 0.9781\n",
      "Epoch 167/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1188 - acc: 0.9754 - val_loss: 0.1533 - val_acc: 0.9792\n",
      "Epoch 168/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1153 - acc: 0.9773 - val_loss: 0.1558 - val_acc: 0.9802\n",
      "Epoch 169/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1197 - acc: 0.9735 - val_loss: 0.1525 - val_acc: 0.9760\n",
      "Epoch 170/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1316 - acc: 0.9725 - val_loss: 0.1566 - val_acc: 0.9781\n",
      "Epoch 171/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1396 - acc: 0.9721 - val_loss: 0.1853 - val_acc: 0.9750\n",
      "Epoch 172/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1445 - acc: 0.9706 - val_loss: 0.1697 - val_acc: 0.9750\n",
      "Epoch 173/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1545 - acc: 0.9664 - val_loss: 0.1779 - val_acc: 0.9771\n",
      "Epoch 174/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1227 - acc: 0.9740 - val_loss: 0.1827 - val_acc: 0.9750\n",
      "Epoch 175/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.1138 - acc: 0.9716 - val_loss: 0.1829 - val_acc: 0.9750\n",
      "Epoch 176/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1293 - acc: 0.9740 - val_loss: 0.1690 - val_acc: 0.9771\n",
      "Epoch 177/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1725 - acc: 0.9716 - val_loss: 0.1840 - val_acc: 0.9760\n",
      "Epoch 178/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.0931 - acc: 0.9754 - val_loss: 0.1565 - val_acc: 0.9781\n",
      "Epoch 179/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.0969 - acc: 0.9792 - val_loss: 0.1756 - val_acc: 0.9771\n",
      "Epoch 180/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1428 - acc: 0.9721 - val_loss: 0.1550 - val_acc: 0.9760\n",
      "Epoch 181/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1327 - acc: 0.9735 - val_loss: 0.1668 - val_acc: 0.9771\n",
      "Epoch 182/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1437 - acc: 0.9740 - val_loss: 0.1896 - val_acc: 0.9740\n",
      "Epoch 183/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1351 - acc: 0.9773 - val_loss: 0.1700 - val_acc: 0.9719\n",
      "Epoch 184/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1070 - acc: 0.9782 - val_loss: 0.1596 - val_acc: 0.9760\n",
      "Epoch 185/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1358 - acc: 0.9763 - val_loss: 0.1733 - val_acc: 0.9729\n",
      "Epoch 186/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1588 - acc: 0.9688 - val_loss: 0.1621 - val_acc: 0.9781\n",
      "Epoch 187/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1653 - acc: 0.9702 - val_loss: 0.1738 - val_acc: 0.9760\n",
      "Epoch 188/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1553 - acc: 0.9773 - val_loss: 0.1706 - val_acc: 0.9802\n",
      "Epoch 189/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1363 - acc: 0.9702 - val_loss: 0.1714 - val_acc: 0.9760\n",
      "Epoch 190/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1461 - acc: 0.9725 - val_loss: 0.1685 - val_acc: 0.9760\n",
      "Epoch 191/250\n",
      "2112/2112 [==============================] - 0s 92us/step - loss: 0.1272 - acc: 0.9768 - val_loss: 0.1804 - val_acc: 0.9792\n",
      "Epoch 192/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.1539 - acc: 0.9740 - val_loss: 0.1655 - val_acc: 0.9771\n",
      "Epoch 193/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1531 - acc: 0.9744 - val_loss: 0.1661 - val_acc: 0.9760\n",
      "Epoch 194/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1203 - acc: 0.9754 - val_loss: 0.1423 - val_acc: 0.9781\n",
      "Epoch 195/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1588 - acc: 0.9721 - val_loss: 0.1442 - val_acc: 0.9812\n",
      "Epoch 196/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1304 - acc: 0.9725 - val_loss: 0.1776 - val_acc: 0.9771\n",
      "Epoch 197/250\n",
      "2112/2112 [==============================] - 0s 102us/step - loss: 0.1271 - acc: 0.9744 - val_loss: 0.1810 - val_acc: 0.9802\n",
      "Epoch 198/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.1411 - acc: 0.9768 - val_loss: 0.1807 - val_acc: 0.9792\n",
      "Epoch 199/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1324 - acc: 0.9754 - val_loss: 0.1769 - val_acc: 0.9729\n",
      "Epoch 200/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1691 - acc: 0.9706 - val_loss: 0.1608 - val_acc: 0.9771\n",
      "Epoch 201/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1599 - acc: 0.9706 - val_loss: 0.1568 - val_acc: 0.9771\n",
      "Epoch 202/250\n",
      "2112/2112 [==============================] - 0s 102us/step - loss: 0.1375 - acc: 0.9792 - val_loss: 0.1844 - val_acc: 0.9708\n",
      "Epoch 203/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1210 - acc: 0.9787 - val_loss: 0.1741 - val_acc: 0.9771\n",
      "Epoch 204/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.1585 - acc: 0.9725 - val_loss: 0.1530 - val_acc: 0.9802\n",
      "Epoch 205/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1214 - acc: 0.9768 - val_loss: 0.1796 - val_acc: 0.9760\n",
      "Epoch 206/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1083 - acc: 0.9763 - val_loss: 0.1558 - val_acc: 0.9792\n",
      "Epoch 207/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.1127 - acc: 0.9759 - val_loss: 0.1426 - val_acc: 0.9823\n",
      "Epoch 208/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1162 - acc: 0.9763 - val_loss: 0.1365 - val_acc: 0.9792\n",
      "Epoch 209/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1394 - acc: 0.9763 - val_loss: 0.1611 - val_acc: 0.9740\n",
      "Epoch 210/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1734 - acc: 0.9697 - val_loss: 0.1655 - val_acc: 0.9792\n",
      "Epoch 211/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1627 - acc: 0.9725 - val_loss: 0.1546 - val_acc: 0.9802\n",
      "Epoch 212/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1297 - acc: 0.9754 - val_loss: 0.1623 - val_acc: 0.9865\n",
      "Epoch 213/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1016 - acc: 0.9792 - val_loss: 0.1524 - val_acc: 0.9812\n",
      "Epoch 214/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1127 - acc: 0.9763 - val_loss: 0.1429 - val_acc: 0.9792\n",
      "Epoch 215/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.1157 - acc: 0.9777 - val_loss: 0.1518 - val_acc: 0.9802\n",
      "Epoch 216/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.1321 - acc: 0.9768 - val_loss: 0.1654 - val_acc: 0.9750\n",
      "Epoch 217/250\n",
      "2112/2112 [==============================] - 0s 105us/step - loss: 0.1562 - acc: 0.9749 - val_loss: 0.1803 - val_acc: 0.9729\n",
      "Epoch 218/250\n",
      "2112/2112 [==============================] - 0s 93us/step - loss: 0.1082 - acc: 0.9782 - val_loss: 0.1629 - val_acc: 0.9781\n",
      "Epoch 219/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1451 - acc: 0.9721 - val_loss: 0.1710 - val_acc: 0.9740\n",
      "Epoch 220/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1133 - acc: 0.9825 - val_loss: 0.1769 - val_acc: 0.9760\n",
      "Epoch 221/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.0970 - acc: 0.9754 - val_loss: 0.1903 - val_acc: 0.9781\n",
      "Epoch 222/250\n",
      "2112/2112 [==============================] - 0s 102us/step - loss: 0.1395 - acc: 0.9706 - val_loss: 0.1800 - val_acc: 0.9750\n",
      "Epoch 223/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1270 - acc: 0.9716 - val_loss: 0.1756 - val_acc: 0.9760\n",
      "Epoch 224/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.1140 - acc: 0.9792 - val_loss: 0.1706 - val_acc: 0.9781\n",
      "Epoch 225/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.1062 - acc: 0.9763 - val_loss: 0.1582 - val_acc: 0.9792\n",
      "Epoch 226/250\n",
      "2112/2112 [==============================] - 0s 102us/step - loss: 0.1238 - acc: 0.9768 - val_loss: 0.1680 - val_acc: 0.9792\n",
      "Epoch 227/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1275 - acc: 0.9759 - val_loss: 0.1509 - val_acc: 0.9781\n",
      "Epoch 228/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1153 - acc: 0.9777 - val_loss: 0.1646 - val_acc: 0.9729\n",
      "Epoch 229/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.1303 - acc: 0.9777 - val_loss: 0.1843 - val_acc: 0.9771\n",
      "Epoch 230/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1016 - acc: 0.9801 - val_loss: 0.1692 - val_acc: 0.9781\n",
      "Epoch 231/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1349 - acc: 0.9730 - val_loss: 0.1672 - val_acc: 0.9750\n",
      "Epoch 232/250\n",
      "2112/2112 [==============================] - 0s 94us/step - loss: 0.1360 - acc: 0.9721 - val_loss: 0.1553 - val_acc: 0.9750\n",
      "Epoch 233/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1203 - acc: 0.9759 - val_loss: 0.1559 - val_acc: 0.9740\n",
      "Epoch 234/250\n",
      "2112/2112 [==============================] - 0s 107us/step - loss: 0.1452 - acc: 0.9716 - val_loss: 0.1561 - val_acc: 0.9771\n",
      "Epoch 235/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1369 - acc: 0.9777 - val_loss: 0.1503 - val_acc: 0.9760\n",
      "Epoch 236/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1498 - acc: 0.9725 - val_loss: 0.1548 - val_acc: 0.9792\n",
      "Epoch 237/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1336 - acc: 0.9792 - val_loss: 0.1623 - val_acc: 0.9802\n",
      "Epoch 238/250\n",
      "2112/2112 [==============================] - 0s 99us/step - loss: 0.1185 - acc: 0.9801 - val_loss: 0.1666 - val_acc: 0.9792\n",
      "Epoch 239/250\n",
      "2112/2112 [==============================] - 0s 102us/step - loss: 0.1332 - acc: 0.9806 - val_loss: 0.1681 - val_acc: 0.9781\n",
      "Epoch 240/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1227 - acc: 0.9754 - val_loss: 0.1653 - val_acc: 0.9781\n",
      "Epoch 241/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1345 - acc: 0.9768 - val_loss: 0.1767 - val_acc: 0.9771\n",
      "Epoch 242/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1746 - acc: 0.9702 - val_loss: 0.1729 - val_acc: 0.9750\n",
      "Epoch 243/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1565 - acc: 0.9768 - val_loss: 0.1651 - val_acc: 0.9802\n",
      "Epoch 244/250\n",
      "2112/2112 [==============================] - 0s 97us/step - loss: 0.0975 - acc: 0.9796 - val_loss: 0.1656 - val_acc: 0.9781\n",
      "Epoch 245/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1250 - acc: 0.9773 - val_loss: 0.1747 - val_acc: 0.9792\n",
      "Epoch 246/250\n",
      "2112/2112 [==============================] - 0s 100us/step - loss: 0.1540 - acc: 0.9725 - val_loss: 0.1630 - val_acc: 0.9771\n",
      "Epoch 247/250\n",
      "2112/2112 [==============================] - 0s 107us/step - loss: 0.1010 - acc: 0.9792 - val_loss: 0.1491 - val_acc: 0.9812\n",
      "Epoch 248/250\n",
      "2112/2112 [==============================] - 0s 96us/step - loss: 0.1332 - acc: 0.9811 - val_loss: 0.1502 - val_acc: 0.9812\n",
      "Epoch 249/250\n",
      "2112/2112 [==============================] - 0s 98us/step - loss: 0.1441 - acc: 0.9725 - val_loss: 0.1669 - val_acc: 0.9771\n",
      "Epoch 250/250\n",
      "2112/2112 [==============================] - 0s 95us/step - loss: 0.1405 - acc: 0.9787 - val_loss: 0.1585 - val_acc: 0.9771\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizers.rmsprop(lr=0.0005, decay=1e-6),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"]) #lr 0.0005\n",
    "\n",
    "# Train the model\n",
    "print(\"[INFO] training model...\")\n",
    "history = model.fit(trainFeatureX, trainY, validation_data=(testFeatureX, testY), epochs=250, batch_size=32)  # 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluating...\n",
      "960/960 [==============================] - 0s 33us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1585460499878915, 0.9770833333333333]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "print(\"[INFO] Evaluating...\")\n",
    "model.evaluate(testFeatureX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training CNN (1-D Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading data...\n",
      "[INFO] processing data...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 277, 512)          1024      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 277, 512)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 141824)            0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 141824)            0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1024)              145228800 \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                8208      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 145,762,832\n",
      "Trainable params: 145,762,832\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(277, 1), filters=512, kernel_size=1)`\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import os\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPool1D\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "print(\"[INFO] loading data...\")\n",
    "#features, images, labels = features, images, labels\n",
    "\n",
    "print(\"[INFO] processing data...\")\n",
    "#split = train_test_split(features, images, labels, test_size=0.25, random_state=42)\n",
    "#(trainFeatureX, testFeatureX, trainImagesX, testImagesX, trainY, testY) = split\n",
    "\n",
    "\n",
    "trainFeatureX, testFeatureX, trainY, testY = trainAudioFeatures, testAudioFeatures, train_labels, test_labels\n",
    "\n",
    "# For prediction and Confusion Matrix\n",
    "trueY = testY\n",
    "\n",
    "trainY = to_categorical(trainY, 16)\n",
    "testY = to_categorical(testY, 16)\n",
    "\n",
    "\n",
    "# define model\n",
    "n_steps = 1\n",
    "n_features = 277\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=(n_steps, n_features)))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(50, activation='relu'))\n",
    "# model.add(Dense(16,activation=\"softmax\"))\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(nb_filter=512, filter_length=1, input_shape=(n_features, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# # create the MLP and CNN models\n",
    "# #mlp = create_mlp()\n",
    "# cnn = create_cnn(120, 1, 1)\n",
    "\n",
    "# # create the input to our final set of layers as the *output* of both the MLP and CNN\n",
    "# #combinedInput = concatenate([mlp.output, cnn.output])\n",
    "\n",
    "# Input = cnn.output\n",
    "\n",
    "# # The final FC layer head will have two dense layers, the final one being softmax layer\n",
    "# x = Dense(512, activation=\"relu\")(Input)\n",
    "# x = Dense(16, activation=\"softmax\")(x)\n",
    "\n",
    "# # The final model will accept audio on the MLP input and SPEC on the CNN input, outputting prediction\n",
    "# model_cnn = Model(inputs=cnn.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Train on 2112 samples, validate on 960 samples\n",
      "Epoch 1/250\n",
      "2112/2112 [==============================] - 7s 3ms/step - loss: 1.6011 - acc: 0.6089 - val_loss: 0.9298 - val_acc: 0.7010\n",
      "Epoch 2/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5094 - acc: 0.8400 - val_loss: 0.3673 - val_acc: 0.8906\n",
      "Epoch 3/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.3411 - acc: 0.9119 - val_loss: 0.2864 - val_acc: 0.9052\n",
      "Epoch 4/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.2183 - acc: 0.9394 - val_loss: 0.2671 - val_acc: 0.9344\n",
      "Epoch 5/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.1968 - acc: 0.9455 - val_loss: 0.2143 - val_acc: 0.9490\n",
      "Epoch 6/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.1262 - acc: 0.9692 - val_loss: 0.1926 - val_acc: 0.9458\n",
      "Epoch 7/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.1083 - acc: 0.9749 - val_loss: 0.1654 - val_acc: 0.9563\n",
      "Epoch 8/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.1028 - acc: 0.9768 - val_loss: 0.1330 - val_acc: 0.9573\n",
      "Epoch 9/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0715 - acc: 0.9863 - val_loss: 0.3052 - val_acc: 0.9302\n",
      "Epoch 10/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0692 - acc: 0.9882 - val_loss: 0.1855 - val_acc: 0.9583\n",
      "Epoch 11/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0813 - acc: 0.9877 - val_loss: 0.1584 - val_acc: 0.9677\n",
      "Epoch 12/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0551 - acc: 0.9920 - val_loss: 0.2024 - val_acc: 0.9604\n",
      "Epoch 13/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0577 - acc: 0.9905 - val_loss: 0.2312 - val_acc: 0.9469\n",
      "Epoch 14/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0503 - acc: 0.9915 - val_loss: 0.1602 - val_acc: 0.9646\n",
      "Epoch 15/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0500 - acc: 0.9929 - val_loss: 0.2096 - val_acc: 0.9604\n",
      "Epoch 16/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0644 - acc: 0.9905 - val_loss: 0.2167 - val_acc: 0.9500\n",
      "Epoch 17/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0535 - acc: 0.9948 - val_loss: 0.2519 - val_acc: 0.9542\n",
      "Epoch 18/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0516 - acc: 0.9948 - val_loss: 0.2528 - val_acc: 0.9604\n",
      "Epoch 19/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0492 - acc: 0.9929 - val_loss: 0.1814 - val_acc: 0.9667\n",
      "Epoch 20/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0503 - acc: 0.9948 - val_loss: 0.1468 - val_acc: 0.9677\n",
      "Epoch 21/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0423 - acc: 0.9943 - val_loss: 0.1136 - val_acc: 0.9771\n",
      "Epoch 22/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0702 - acc: 0.9910 - val_loss: 0.1497 - val_acc: 0.9698\n",
      "Epoch 23/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0390 - acc: 0.9943 - val_loss: 0.3552 - val_acc: 0.9490\n",
      "Epoch 24/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0446 - acc: 0.9953 - val_loss: 0.2046 - val_acc: 0.9708\n",
      "Epoch 25/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0343 - acc: 0.9967 - val_loss: 0.1881 - val_acc: 0.9677\n",
      "Epoch 26/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0466 - acc: 0.9962 - val_loss: 0.2065 - val_acc: 0.9646\n",
      "Epoch 27/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0459 - acc: 0.9934 - val_loss: 0.2965 - val_acc: 0.9490\n",
      "Epoch 28/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0415 - acc: 0.9953 - val_loss: 0.2996 - val_acc: 0.9573\n",
      "Epoch 29/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0389 - acc: 0.9967 - val_loss: 0.2033 - val_acc: 0.9635\n",
      "Epoch 30/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0349 - acc: 0.9967 - val_loss: 0.1918 - val_acc: 0.9719\n",
      "Epoch 31/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0391 - acc: 0.9967 - val_loss: 0.1990 - val_acc: 0.9698\n",
      "Epoch 32/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0378 - acc: 0.9957 - val_loss: 0.1967 - val_acc: 0.9698\n",
      "Epoch 33/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0307 - acc: 0.9981 - val_loss: 0.2077 - val_acc: 0.9750\n",
      "Epoch 34/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0488 - acc: 0.9953 - val_loss: 0.2295 - val_acc: 0.9688\n",
      "Epoch 35/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0340 - acc: 0.9976 - val_loss: 0.3831 - val_acc: 0.9510\n",
      "Epoch 36/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0360 - acc: 0.9967 - val_loss: 0.2699 - val_acc: 0.9667\n",
      "Epoch 37/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0328 - acc: 0.9976 - val_loss: 0.2221 - val_acc: 0.9698\n",
      "Epoch 38/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0421 - acc: 0.9962 - val_loss: 0.2133 - val_acc: 0.9719\n",
      "Epoch 39/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2129 - val_acc: 0.9719\n",
      "Epoch 40/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2136 - val_acc: 0.9740\n",
      "Epoch 41/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0437 - acc: 0.9972 - val_loss: 0.2612 - val_acc: 0.9708\n",
      "Epoch 42/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0471 - acc: 0.9962 - val_loss: 0.2036 - val_acc: 0.9771\n",
      "Epoch 43/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2160 - val_acc: 0.9750\n",
      "Epoch 44/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2213 - val_acc: 0.9719\n",
      "Epoch 45/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2226 - val_acc: 0.9719\n",
      "Epoch 46/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2276 - val_acc: 0.9729\n",
      "Epoch 47/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2319 - val_acc: 0.9740\n",
      "Epoch 48/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2319 - val_acc: 0.9740\n",
      "Epoch 49/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2400 - val_acc: 0.9729\n",
      "Epoch 50/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0310 - acc: 0.9976 - val_loss: 0.2595 - val_acc: 0.9688\n",
      "Epoch 51/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0331 - acc: 0.9976 - val_loss: 0.2379 - val_acc: 0.9719\n",
      "Epoch 52/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2536 - val_acc: 0.9708\n",
      "Epoch 53/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2554 - val_acc: 0.9688\n",
      "Epoch 54/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2303 - val_acc: 0.9729\n",
      "Epoch 55/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2569 - val_acc: 0.9729\n",
      "Epoch 56/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2520 - val_acc: 0.9729\n",
      "Epoch 57/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2508 - val_acc: 0.9740\n",
      "Epoch 58/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2426 - val_acc: 0.9750\n",
      "Epoch 59/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2426 - val_acc: 0.9750\n",
      "Epoch 60/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2426 - val_acc: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2426 - val_acc: 0.9750\n",
      "Epoch 62/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2426 - val_acc: 0.9750\n",
      "Epoch 63/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2387 - val_acc: 0.9740\n",
      "Epoch 64/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2387 - val_acc: 0.9740\n",
      "Epoch 65/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2387 - val_acc: 0.9740\n",
      "Epoch 66/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2387 - val_acc: 0.9740\n",
      "Epoch 67/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2387 - val_acc: 0.9740\n",
      "Epoch 68/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2387 - val_acc: 0.9740\n",
      "Epoch 69/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2387 - val_acc: 0.9740\n",
      "Epoch 70/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2402 - val_acc: 0.9750\n",
      "Epoch 71/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2402 - val_acc: 0.9750\n",
      "Epoch 72/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2402 - val_acc: 0.9750\n",
      "Epoch 73/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2402 - val_acc: 0.9750\n",
      "Epoch 74/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2402 - val_acc: 0.9750\n",
      "Epoch 75/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2402 - val_acc: 0.9750\n",
      "Epoch 76/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2402 - val_acc: 0.9750\n",
      "Epoch 77/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2402 - val_acc: 0.9750\n",
      "Epoch 78/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2414 - val_acc: 0.9729\n",
      "Epoch 79/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2414 - val_acc: 0.9729\n",
      "Epoch 80/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2414 - val_acc: 0.9729\n",
      "Epoch 81/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2402 - val_acc: 0.9740\n",
      "Epoch 82/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2402 - val_acc: 0.9740\n",
      "Epoch 83/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 84/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 85/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 86/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 87/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 88/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 89/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 90/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 91/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 92/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 93/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 94/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 95/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 96/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 97/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 98/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 99/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 100/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 101/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 102/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 103/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 104/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 105/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 106/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 107/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 108/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 109/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 110/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 111/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 112/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 113/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2408 - val_acc: 0.9740\n",
      "Epoch 114/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2382 - val_acc: 0.9729\n",
      "Epoch 115/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2382 - val_acc: 0.9729\n",
      "Epoch 116/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2382 - val_acc: 0.9729\n",
      "Epoch 117/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2382 - val_acc: 0.9729\n",
      "Epoch 118/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2382 - val_acc: 0.9729\n",
      "Epoch 119/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2382 - val_acc: 0.9729\n",
      "Epoch 120/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2382 - val_acc: 0.9729\n",
      "Epoch 121/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 122/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 123/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 124/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 125/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 126/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 127/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 128/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 129/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 130/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 131/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 132/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 133/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 134/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 135/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 136/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 137/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 138/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 139/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 140/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 141/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 142/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 143/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 144/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 145/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 146/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 147/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 148/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 149/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 150/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 151/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 152/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 153/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 154/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 155/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 156/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 157/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 158/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 159/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 160/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 161/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 162/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 163/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 164/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 165/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 166/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 167/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 168/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 169/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 170/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 171/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 172/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 173/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 174/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 175/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 176/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 177/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 178/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 179/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 180/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 182/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 183/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 184/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 185/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 186/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 187/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 188/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 189/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 190/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 191/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 192/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 193/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 194/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 195/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 196/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 197/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 198/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 199/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 200/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 201/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 202/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 203/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 204/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 205/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 206/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 207/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 208/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 209/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 210/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 211/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 212/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 213/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 214/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 215/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 216/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 217/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 218/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 219/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 220/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 221/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2441 - val_acc: 0.9740\n",
      "Epoch 222/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 223/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 224/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 225/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 226/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 227/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 228/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 229/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 230/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 231/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 232/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 233/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 234/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 235/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 236/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 237/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 238/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 239/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 240/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 242/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 243/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 244/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 245/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 246/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 247/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 248/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 249/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n",
      "Epoch 250/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.0305 - acc: 0.9981 - val_loss: 0.2440 - val_acc: 0.9740\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizers.rmsprop(lr=0.0005, decay=1e-6),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"]) #lr 0.0005\n",
    "\n",
    "# Train the model\n",
    "print(\"[INFO] training model...\")\n",
    "history = model.fit(trainFeatureX.reshape(2112, 277,1), trainY, validation_data=(testFeatureX.reshape(960, 277,1), testY), epochs=250, batch_size=32)  # 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8925263570212446872\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6622735237\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17406887731317095264\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.python.client import device_lib\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"99\"\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluating...\n",
      "960/960 [==============================] - 0s 138us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24400023853540442, 0.9739583333333334]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "print(\"[INFO] Evaluating...\")\n",
    "model.evaluate(testFeatureX.reshape(960,277,1), testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN (1-D Feature: Mel-Spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction: Mean of Mel-Spec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting Features...\n",
      "Finished feature extraction from  2112 files\n",
      "The array is saved in the .npy file\n"
     ]
    }
   ],
   "source": [
    "traindf = pd.read_csv('train.csv',dtype=str)\n",
    "trainAudioPath = \"ALL_A_TRAIN/\"\n",
    "\n",
    "mel_features = []\n",
    "print(\"[INFO] Extracting Features...\")\n",
    "for fileID in traindf[\"ID\"]:\n",
    "    audioFile = fileID + '.wav'\n",
    "    audioDir = trainAudioPath + audioFile\n",
    "    #print(audioPath)\n",
    "    data = extract_features_spec(audioDir)\n",
    "    mel_features.append(data)\n",
    "    \n",
    "print('Finished feature extraction from ', len(mel_features), 'files') \n",
    "\n",
    "# Save to binary file\n",
    "np.save('trainMelFeature', mel_features) \n",
    "print(\"The array is saved in the .npy file\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Extracting Features...\n",
      "Finished feature extraction from  960 files\n",
      "The array is saved in the .npy file\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv('test.csv',dtype=str)\n",
    "trainAudioPath = \"ALL_A_TEST/\"\n",
    "\n",
    "mel2_features = []\n",
    "print(\"[INFO] Extracting Features...\")\n",
    "for fileID in testdf[\"ID\"]:\n",
    "    audioFile = fileID + '.wav'\n",
    "    audioDir = trainAudioPath + audioFile\n",
    "    #print(audioPath)\n",
    "    data = extract_features_spec(audioDir)\n",
    "    mel2_features.append(data)\n",
    "    \n",
    "print('Finished feature extraction from ', len(mel2_features), 'files') \n",
    "\n",
    "# Save to binary file\n",
    "np.save('testMelFeature', mel2_features) \n",
    "print(\"The array is saved in the .npy file\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk \n",
    "\n",
    "trainMelFeatures = np.load('trainMelFeature.npy', allow_pickle=True)\n",
    "testMelFeatures = np.load('testMelFeature.npy', allow_pickle=True)\n",
    "\n",
    "trainMelFeatures = np.array(trainMelFeatures)\n",
    "testMelFeatures = np.array(testMelFeatures)\n",
    "\n",
    "scaler1 = sk.preprocessing.StandardScaler().fit(trainMelFeatures)\n",
    "trainMelFeatures = scaler1.transform(trainMelFeatures)\n",
    "testMelFeatures = scaler1.transform(testMelFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 128)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMelFeatures.shape\n",
    "testMelFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "train_labels = traindf[\"Class\"]\n",
    "test_labels = testdf[\"Class\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading data...\n",
      "[INFO] processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(128, 1), filters=512, kernel_size=1)`\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import os\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPool1D\n",
    "from keras import regularizers, optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "print(\"[INFO] loading data...\")\n",
    "#features, images, labels = features, images, labels\n",
    "\n",
    "print(\"[INFO] processing data...\")\n",
    "#split = train_test_split(features, images, labels, test_size=0.25, random_state=42)\n",
    "#(trainFeatureX, testFeatureX, trainImagesX, testImagesX, trainY, testY) = split\n",
    "\n",
    "\n",
    "trainFeatureX, testFeatureX, trainY, testY = trainMelFeatures, testMelFeatures, train_labels, test_labels\n",
    "\n",
    "# For prediction and Confusion Matrix\n",
    "trueY = testY\n",
    "\n",
    "trainY = to_categorical(trainY, 16)\n",
    "testY = to_categorical(testY, 16)\n",
    "\n",
    "\n",
    "# define model\n",
    "n_steps = 1\n",
    "n_features = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(nb_filter=512, filter_length=1, input_shape=(n_features, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Train on 2112 samples, validate on 960 samples\n",
      "Epoch 1/250\n",
      "2112/2112 [==============================] - 6s 3ms/step - loss: 2.1440 - acc: 0.3703 - val_loss: 1.7032 - val_acc: 0.4927\n",
      "Epoch 2/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 1.3640 - acc: 0.5800 - val_loss: 1.2359 - val_acc: 0.5990\n",
      "Epoch 3/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 1.1884 - acc: 0.6179 - val_loss: 1.0591 - val_acc: 0.6604\n",
      "Epoch 4/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 1.0627 - acc: 0.6553 - val_loss: 1.0682 - val_acc: 0.6531\n",
      "Epoch 5/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 1.0003 - acc: 0.6705 - val_loss: 1.0031 - val_acc: 0.6708\n",
      "Epoch 6/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.9571 - acc: 0.6832 - val_loss: 1.0914 - val_acc: 0.6687\n",
      "Epoch 7/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.9165 - acc: 0.6927 - val_loss: 1.0226 - val_acc: 0.6698\n",
      "Epoch 8/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.8879 - acc: 0.7116 - val_loss: 0.9241 - val_acc: 0.7208\n",
      "Epoch 9/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.8554 - acc: 0.7173 - val_loss: 0.9302 - val_acc: 0.7073\n",
      "Epoch 10/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.8277 - acc: 0.7225 - val_loss: 0.9575 - val_acc: 0.6990\n",
      "Epoch 11/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.8183 - acc: 0.7292 - val_loss: 0.9311 - val_acc: 0.7146\n",
      "Epoch 12/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.7870 - acc: 0.7401 - val_loss: 0.9215 - val_acc: 0.7219\n",
      "Epoch 13/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.7857 - acc: 0.7528 - val_loss: 0.9802 - val_acc: 0.7063\n",
      "Epoch 14/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.7420 - acc: 0.7533 - val_loss: 1.1328 - val_acc: 0.6604\n",
      "Epoch 15/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.7443 - acc: 0.7500 - val_loss: 0.8167 - val_acc: 0.7365\n",
      "Epoch 16/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.7317 - acc: 0.7552 - val_loss: 0.8828 - val_acc: 0.7271\n",
      "Epoch 17/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.7161 - acc: 0.7628 - val_loss: 0.8602 - val_acc: 0.7344\n",
      "Epoch 18/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.7235 - acc: 0.7566 - val_loss: 0.9058 - val_acc: 0.7031\n",
      "Epoch 19/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.7123 - acc: 0.7604 - val_loss: 0.9120 - val_acc: 0.7208\n",
      "Epoch 20/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.6915 - acc: 0.7647 - val_loss: 0.8698 - val_acc: 0.7135\n",
      "Epoch 21/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.6804 - acc: 0.7741 - val_loss: 0.9302 - val_acc: 0.7188\n",
      "Epoch 22/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.6708 - acc: 0.7741 - val_loss: 0.8951 - val_acc: 0.7188\n",
      "Epoch 23/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6479 - acc: 0.7779 - val_loss: 0.9840 - val_acc: 0.7219\n",
      "Epoch 24/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6538 - acc: 0.7874 - val_loss: 0.8123 - val_acc: 0.7458\n",
      "Epoch 25/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6353 - acc: 0.7817 - val_loss: 0.8448 - val_acc: 0.7583\n",
      "Epoch 26/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6617 - acc: 0.7827 - val_loss: 0.9157 - val_acc: 0.7292\n",
      "Epoch 27/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6506 - acc: 0.7775 - val_loss: 0.8787 - val_acc: 0.7375\n",
      "Epoch 28/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6254 - acc: 0.7874 - val_loss: 0.7925 - val_acc: 0.7719\n",
      "Epoch 29/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6131 - acc: 0.7827 - val_loss: 0.8204 - val_acc: 0.7719\n",
      "Epoch 30/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6055 - acc: 0.7959 - val_loss: 0.8953 - val_acc: 0.7281\n",
      "Epoch 31/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6130 - acc: 0.7888 - val_loss: 0.8324 - val_acc: 0.7646\n",
      "Epoch 32/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6033 - acc: 0.7888 - val_loss: 0.8542 - val_acc: 0.7469\n",
      "Epoch 33/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6244 - acc: 0.7902 - val_loss: 0.8485 - val_acc: 0.7635\n",
      "Epoch 34/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6116 - acc: 0.7936 - val_loss: 0.9176 - val_acc: 0.7323\n",
      "Epoch 35/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6022 - acc: 0.7940 - val_loss: 0.8021 - val_acc: 0.7667\n",
      "Epoch 36/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6206 - acc: 0.7907 - val_loss: 0.8965 - val_acc: 0.7292\n",
      "Epoch 37/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.6152 - acc: 0.7940 - val_loss: 0.8250 - val_acc: 0.7771\n",
      "Epoch 38/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5893 - acc: 0.8068 - val_loss: 0.8629 - val_acc: 0.7667\n",
      "Epoch 39/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5706 - acc: 0.8059 - val_loss: 0.8045 - val_acc: 0.7667\n",
      "Epoch 40/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5699 - acc: 0.8082 - val_loss: 0.9148 - val_acc: 0.7417\n",
      "Epoch 41/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5884 - acc: 0.7921 - val_loss: 0.9968 - val_acc: 0.7271\n",
      "Epoch 42/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5571 - acc: 0.8045 - val_loss: 1.0357 - val_acc: 0.7458\n",
      "Epoch 43/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5557 - acc: 0.8116 - val_loss: 0.8306 - val_acc: 0.7698\n",
      "Epoch 44/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5450 - acc: 0.8134 - val_loss: 0.9039 - val_acc: 0.7604\n",
      "Epoch 45/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5459 - acc: 0.8092 - val_loss: 0.9422 - val_acc: 0.7688\n",
      "Epoch 46/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5500 - acc: 0.8106 - val_loss: 0.9157 - val_acc: 0.7635\n",
      "Epoch 47/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5669 - acc: 0.8158 - val_loss: 0.8803 - val_acc: 0.7875\n",
      "Epoch 48/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5819 - acc: 0.8063 - val_loss: 1.0848 - val_acc: 0.7417\n",
      "Epoch 49/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5732 - acc: 0.8111 - val_loss: 0.9261 - val_acc: 0.7708\n",
      "Epoch 50/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5780 - acc: 0.8116 - val_loss: 1.0728 - val_acc: 0.7458\n",
      "Epoch 51/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5356 - acc: 0.8187 - val_loss: 0.9539 - val_acc: 0.7615\n",
      "Epoch 52/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5418 - acc: 0.8092 - val_loss: 0.9009 - val_acc: 0.7781\n",
      "Epoch 53/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5356 - acc: 0.8191 - val_loss: 0.8396 - val_acc: 0.7771\n",
      "Epoch 54/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5439 - acc: 0.8196 - val_loss: 0.8982 - val_acc: 0.7604\n",
      "Epoch 55/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5372 - acc: 0.8149 - val_loss: 0.8449 - val_acc: 0.7906\n",
      "Epoch 56/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5680 - acc: 0.8158 - val_loss: 0.9539 - val_acc: 0.7885\n",
      "Epoch 57/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5739 - acc: 0.8144 - val_loss: 0.7973 - val_acc: 0.7688\n",
      "Epoch 58/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5286 - acc: 0.8182 - val_loss: 1.0583 - val_acc: 0.7573\n",
      "Epoch 59/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5338 - acc: 0.8191 - val_loss: 0.7573 - val_acc: 0.7885\n",
      "Epoch 60/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5618 - acc: 0.8144 - val_loss: 0.9257 - val_acc: 0.7844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5483 - acc: 0.8196 - val_loss: 0.8770 - val_acc: 0.7885\n",
      "Epoch 62/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5334 - acc: 0.8153 - val_loss: 0.8899 - val_acc: 0.7740\n",
      "Epoch 63/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5494 - acc: 0.8158 - val_loss: 0.9715 - val_acc: 0.7656\n",
      "Epoch 64/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5102 - acc: 0.8295 - val_loss: 0.8092 - val_acc: 0.7958\n",
      "Epoch 65/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5093 - acc: 0.8286 - val_loss: 1.1074 - val_acc: 0.7479\n",
      "Epoch 66/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5009 - acc: 0.8262 - val_loss: 0.9055 - val_acc: 0.7740\n",
      "Epoch 67/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5284 - acc: 0.8253 - val_loss: 0.9818 - val_acc: 0.7625\n",
      "Epoch 68/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5229 - acc: 0.8248 - val_loss: 0.8390 - val_acc: 0.7906\n",
      "Epoch 69/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5483 - acc: 0.8215 - val_loss: 0.8345 - val_acc: 0.7812\n",
      "Epoch 70/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5369 - acc: 0.8187 - val_loss: 0.8863 - val_acc: 0.7708\n",
      "Epoch 71/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5119 - acc: 0.8272 - val_loss: 0.8777 - val_acc: 0.7990\n",
      "Epoch 72/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5125 - acc: 0.8333 - val_loss: 0.8889 - val_acc: 0.7667\n",
      "Epoch 73/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5290 - acc: 0.8258 - val_loss: 0.8624 - val_acc: 0.7906\n",
      "Epoch 74/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5082 - acc: 0.8310 - val_loss: 0.9436 - val_acc: 0.7812\n",
      "Epoch 75/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5213 - acc: 0.8286 - val_loss: 0.9623 - val_acc: 0.7562\n",
      "Epoch 76/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5171 - acc: 0.8343 - val_loss: 0.8905 - val_acc: 0.7802\n",
      "Epoch 77/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5064 - acc: 0.8248 - val_loss: 0.9057 - val_acc: 0.7677\n",
      "Epoch 78/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4737 - acc: 0.8314 - val_loss: 0.9744 - val_acc: 0.7740\n",
      "Epoch 79/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4928 - acc: 0.8343 - val_loss: 0.9153 - val_acc: 0.7896\n",
      "Epoch 80/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4875 - acc: 0.8329 - val_loss: 0.9677 - val_acc: 0.7635\n",
      "Epoch 81/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4902 - acc: 0.8314 - val_loss: 0.9149 - val_acc: 0.7792\n",
      "Epoch 82/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4995 - acc: 0.8277 - val_loss: 1.0153 - val_acc: 0.7688\n",
      "Epoch 83/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5215 - acc: 0.8243 - val_loss: 0.9023 - val_acc: 0.7927\n",
      "Epoch 84/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4761 - acc: 0.8362 - val_loss: 0.8579 - val_acc: 0.7833\n",
      "Epoch 85/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5103 - acc: 0.8295 - val_loss: 0.8610 - val_acc: 0.7812\n",
      "Epoch 86/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5089 - acc: 0.8272 - val_loss: 0.8693 - val_acc: 0.7750\n",
      "Epoch 87/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4812 - acc: 0.8333 - val_loss: 0.9197 - val_acc: 0.7833\n",
      "Epoch 88/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5504 - acc: 0.8300 - val_loss: 1.1319 - val_acc: 0.7573\n",
      "Epoch 89/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5047 - acc: 0.8248 - val_loss: 1.0693 - val_acc: 0.7698\n",
      "Epoch 90/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5093 - acc: 0.8262 - val_loss: 0.9382 - val_acc: 0.7844\n",
      "Epoch 91/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4989 - acc: 0.8295 - val_loss: 0.8988 - val_acc: 0.7823\n",
      "Epoch 92/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5292 - acc: 0.8262 - val_loss: 1.1283 - val_acc: 0.7771\n",
      "Epoch 93/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4922 - acc: 0.8442 - val_loss: 0.9479 - val_acc: 0.7875\n",
      "Epoch 94/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4944 - acc: 0.8281 - val_loss: 0.9814 - val_acc: 0.7792\n",
      "Epoch 95/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4695 - acc: 0.8390 - val_loss: 0.9547 - val_acc: 0.7906\n",
      "Epoch 96/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5280 - acc: 0.8272 - val_loss: 0.9226 - val_acc: 0.7812\n",
      "Epoch 97/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5255 - acc: 0.8281 - val_loss: 0.9222 - val_acc: 0.7729\n",
      "Epoch 98/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4916 - acc: 0.8381 - val_loss: 0.8894 - val_acc: 0.7802\n",
      "Epoch 99/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5073 - acc: 0.8352 - val_loss: 1.2079 - val_acc: 0.7698\n",
      "Epoch 100/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5153 - acc: 0.8409 - val_loss: 0.9546 - val_acc: 0.7802\n",
      "Epoch 101/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4980 - acc: 0.8343 - val_loss: 1.1748 - val_acc: 0.7771\n",
      "Epoch 102/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5080 - acc: 0.8272 - val_loss: 0.9380 - val_acc: 0.7958\n",
      "Epoch 103/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4560 - acc: 0.8366 - val_loss: 0.9362 - val_acc: 0.7906\n",
      "Epoch 104/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4779 - acc: 0.8357 - val_loss: 0.9486 - val_acc: 0.7719\n",
      "Epoch 105/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5294 - acc: 0.8343 - val_loss: 1.2166 - val_acc: 0.7490\n",
      "Epoch 106/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5183 - acc: 0.8362 - val_loss: 1.0546 - val_acc: 0.7854\n",
      "Epoch 107/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4925 - acc: 0.8376 - val_loss: 0.9665 - val_acc: 0.7896\n",
      "Epoch 108/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4837 - acc: 0.8366 - val_loss: 0.9549 - val_acc: 0.7906\n",
      "Epoch 109/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4765 - acc: 0.8362 - val_loss: 0.9412 - val_acc: 0.7937\n",
      "Epoch 110/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5121 - acc: 0.8376 - val_loss: 0.8993 - val_acc: 0.8000\n",
      "Epoch 111/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4727 - acc: 0.8442 - val_loss: 0.8973 - val_acc: 0.8031\n",
      "Epoch 112/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4897 - acc: 0.8352 - val_loss: 0.8967 - val_acc: 0.7906\n",
      "Epoch 113/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4784 - acc: 0.8438 - val_loss: 0.9924 - val_acc: 0.7823\n",
      "Epoch 114/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5191 - acc: 0.8456 - val_loss: 1.1526 - val_acc: 0.7552\n",
      "Epoch 115/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4914 - acc: 0.8438 - val_loss: 1.0539 - val_acc: 0.7500\n",
      "Epoch 116/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4890 - acc: 0.8376 - val_loss: 0.9968 - val_acc: 0.7812\n",
      "Epoch 117/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4671 - acc: 0.8419 - val_loss: 0.8965 - val_acc: 0.7833\n",
      "Epoch 118/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4883 - acc: 0.8314 - val_loss: 0.9650 - val_acc: 0.7708\n",
      "Epoch 119/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4563 - acc: 0.8423 - val_loss: 0.9849 - val_acc: 0.7833\n",
      "Epoch 120/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4699 - acc: 0.8447 - val_loss: 0.8561 - val_acc: 0.8021\n",
      "Epoch 121/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4564 - acc: 0.8447 - val_loss: 0.8813 - val_acc: 0.7979\n",
      "Epoch 122/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4527 - acc: 0.8423 - val_loss: 0.9809 - val_acc: 0.7885\n",
      "Epoch 123/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4796 - acc: 0.8352 - val_loss: 1.0019 - val_acc: 0.7750\n",
      "Epoch 124/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4715 - acc: 0.8395 - val_loss: 1.0470 - val_acc: 0.7812\n",
      "Epoch 125/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4927 - acc: 0.8442 - val_loss: 1.0475 - val_acc: 0.7781\n",
      "Epoch 126/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4530 - acc: 0.8423 - val_loss: 1.2590 - val_acc: 0.7542\n",
      "Epoch 127/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4877 - acc: 0.8390 - val_loss: 1.0011 - val_acc: 0.7823\n",
      "Epoch 128/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4725 - acc: 0.8338 - val_loss: 1.0117 - val_acc: 0.7760\n",
      "Epoch 129/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4463 - acc: 0.8466 - val_loss: 0.9382 - val_acc: 0.7781\n",
      "Epoch 130/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4552 - acc: 0.8419 - val_loss: 0.9161 - val_acc: 0.7729\n",
      "Epoch 131/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5393 - acc: 0.8348 - val_loss: 0.9689 - val_acc: 0.7948\n",
      "Epoch 132/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5009 - acc: 0.8452 - val_loss: 0.9855 - val_acc: 0.7906\n",
      "Epoch 133/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5016 - acc: 0.8438 - val_loss: 0.9793 - val_acc: 0.7865\n",
      "Epoch 134/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4703 - acc: 0.8409 - val_loss: 0.8594 - val_acc: 0.8094\n",
      "Epoch 135/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4778 - acc: 0.8423 - val_loss: 1.0347 - val_acc: 0.8000\n",
      "Epoch 136/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5417 - acc: 0.8348 - val_loss: 1.1132 - val_acc: 0.7812\n",
      "Epoch 137/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4953 - acc: 0.8438 - val_loss: 0.9556 - val_acc: 0.7937\n",
      "Epoch 138/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4936 - acc: 0.8414 - val_loss: 0.9906 - val_acc: 0.8010\n",
      "Epoch 139/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5130 - acc: 0.8404 - val_loss: 1.0891 - val_acc: 0.7865\n",
      "Epoch 140/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4850 - acc: 0.8362 - val_loss: 1.0117 - val_acc: 0.7937\n",
      "Epoch 141/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5014 - acc: 0.8461 - val_loss: 0.9247 - val_acc: 0.8115\n",
      "Epoch 142/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4432 - acc: 0.8527 - val_loss: 0.9587 - val_acc: 0.8010\n",
      "Epoch 143/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4625 - acc: 0.8419 - val_loss: 1.0502 - val_acc: 0.7885\n",
      "Epoch 144/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4468 - acc: 0.8499 - val_loss: 0.9802 - val_acc: 0.7917\n",
      "Epoch 145/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4378 - acc: 0.8461 - val_loss: 0.9810 - val_acc: 0.7875\n",
      "Epoch 146/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4569 - acc: 0.8527 - val_loss: 0.9615 - val_acc: 0.8042\n",
      "Epoch 147/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4439 - acc: 0.8419 - val_loss: 0.9426 - val_acc: 0.7958\n",
      "Epoch 148/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5080 - acc: 0.8414 - val_loss: 1.0282 - val_acc: 0.7792\n",
      "Epoch 149/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4887 - acc: 0.8385 - val_loss: 1.0866 - val_acc: 0.7781\n",
      "Epoch 150/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4818 - acc: 0.8428 - val_loss: 0.9536 - val_acc: 0.7896\n",
      "Epoch 151/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4924 - acc: 0.8423 - val_loss: 0.9613 - val_acc: 0.7865\n",
      "Epoch 152/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4305 - acc: 0.8504 - val_loss: 1.0875 - val_acc: 0.7688\n",
      "Epoch 153/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4513 - acc: 0.8523 - val_loss: 0.9532 - val_acc: 0.8000\n",
      "Epoch 154/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4401 - acc: 0.8504 - val_loss: 0.9498 - val_acc: 0.8135\n",
      "Epoch 155/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4996 - acc: 0.8466 - val_loss: 1.0455 - val_acc: 0.7854\n",
      "Epoch 156/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4375 - acc: 0.8428 - val_loss: 1.0665 - val_acc: 0.7948\n",
      "Epoch 157/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4388 - acc: 0.8485 - val_loss: 1.0059 - val_acc: 0.7885\n",
      "Epoch 158/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5331 - acc: 0.8428 - val_loss: 1.2000 - val_acc: 0.7781\n",
      "Epoch 159/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5584 - acc: 0.8499 - val_loss: 0.9590 - val_acc: 0.8000\n",
      "Epoch 160/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4662 - acc: 0.8494 - val_loss: 1.2837 - val_acc: 0.7729\n",
      "Epoch 161/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5486 - acc: 0.8433 - val_loss: 0.9665 - val_acc: 0.7937\n",
      "Epoch 162/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4646 - acc: 0.8499 - val_loss: 1.1242 - val_acc: 0.7781\n",
      "Epoch 163/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4621 - acc: 0.8471 - val_loss: 1.0707 - val_acc: 0.7719\n",
      "Epoch 164/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4795 - acc: 0.8452 - val_loss: 1.1445 - val_acc: 0.7854\n",
      "Epoch 165/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4743 - acc: 0.8447 - val_loss: 1.0310 - val_acc: 0.7906\n",
      "Epoch 166/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4525 - acc: 0.8499 - val_loss: 0.8997 - val_acc: 0.8167\n",
      "Epoch 167/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4427 - acc: 0.8532 - val_loss: 1.0545 - val_acc: 0.7708\n",
      "Epoch 168/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4674 - acc: 0.8509 - val_loss: 0.9220 - val_acc: 0.8073\n",
      "Epoch 169/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4288 - acc: 0.8513 - val_loss: 0.9921 - val_acc: 0.8042\n",
      "Epoch 170/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4798 - acc: 0.8447 - val_loss: 0.9774 - val_acc: 0.7990\n",
      "Epoch 171/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4595 - acc: 0.8527 - val_loss: 0.9453 - val_acc: 0.7979\n",
      "Epoch 172/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5110 - acc: 0.8475 - val_loss: 1.2159 - val_acc: 0.7615\n",
      "Epoch 173/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.5553 - acc: 0.8490 - val_loss: 1.0332 - val_acc: 0.7937\n",
      "Epoch 174/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4631 - acc: 0.8584 - val_loss: 1.0393 - val_acc: 0.7802\n",
      "Epoch 175/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4749 - acc: 0.8494 - val_loss: 1.0579 - val_acc: 0.7698\n",
      "Epoch 176/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4627 - acc: 0.8480 - val_loss: 1.0951 - val_acc: 0.7833\n",
      "Epoch 177/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4663 - acc: 0.8527 - val_loss: 1.0978 - val_acc: 0.7990\n",
      "Epoch 178/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4278 - acc: 0.8504 - val_loss: 1.0363 - val_acc: 0.7865\n",
      "Epoch 179/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4881 - acc: 0.8414 - val_loss: 1.0964 - val_acc: 0.7906\n",
      "Epoch 180/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4580 - acc: 0.8542 - val_loss: 1.1659 - val_acc: 0.7667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4440 - acc: 0.8461 - val_loss: 1.0743 - val_acc: 0.7917\n",
      "Epoch 182/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4456 - acc: 0.8428 - val_loss: 0.9965 - val_acc: 0.7990\n",
      "Epoch 183/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4533 - acc: 0.8499 - val_loss: 1.0816 - val_acc: 0.7812\n",
      "Epoch 184/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4696 - acc: 0.8447 - val_loss: 0.9846 - val_acc: 0.8042\n",
      "Epoch 185/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4578 - acc: 0.8513 - val_loss: 1.0110 - val_acc: 0.8135\n",
      "Epoch 186/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4811 - acc: 0.8409 - val_loss: 1.0581 - val_acc: 0.7833\n",
      "Epoch 187/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4916 - acc: 0.8428 - val_loss: 1.4093 - val_acc: 0.7750\n",
      "Epoch 188/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4947 - acc: 0.8490 - val_loss: 1.0254 - val_acc: 0.8042\n",
      "Epoch 189/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4795 - acc: 0.8438 - val_loss: 1.1402 - val_acc: 0.7854\n",
      "Epoch 190/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4740 - acc: 0.8480 - val_loss: 1.1893 - val_acc: 0.7896\n",
      "Epoch 191/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4518 - acc: 0.8480 - val_loss: 1.0180 - val_acc: 0.7896\n",
      "Epoch 192/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4093 - acc: 0.8594 - val_loss: 1.0709 - val_acc: 0.7979\n",
      "Epoch 193/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4634 - acc: 0.8499 - val_loss: 0.9647 - val_acc: 0.8104\n",
      "Epoch 194/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4827 - acc: 0.8580 - val_loss: 1.2643 - val_acc: 0.7802\n",
      "Epoch 195/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5129 - acc: 0.8494 - val_loss: 1.2208 - val_acc: 0.7750\n",
      "Epoch 196/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5038 - acc: 0.8485 - val_loss: 0.9666 - val_acc: 0.8031\n",
      "Epoch 197/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4928 - acc: 0.8565 - val_loss: 1.2299 - val_acc: 0.7896\n",
      "Epoch 198/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5304 - acc: 0.8452 - val_loss: 1.0226 - val_acc: 0.8010\n",
      "Epoch 199/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5062 - acc: 0.8461 - val_loss: 1.1649 - val_acc: 0.7917\n",
      "Epoch 200/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4655 - acc: 0.8575 - val_loss: 1.1481 - val_acc: 0.7969\n",
      "Epoch 201/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4609 - acc: 0.8532 - val_loss: 1.0715 - val_acc: 0.7885\n",
      "Epoch 202/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5015 - acc: 0.8556 - val_loss: 1.3486 - val_acc: 0.7698\n",
      "Epoch 203/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5101 - acc: 0.8499 - val_loss: 1.0761 - val_acc: 0.7865\n",
      "Epoch 204/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5165 - acc: 0.8532 - val_loss: 1.1022 - val_acc: 0.7958\n",
      "Epoch 205/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4813 - acc: 0.8527 - val_loss: 1.1768 - val_acc: 0.7688\n",
      "Epoch 206/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4713 - acc: 0.8537 - val_loss: 1.1025 - val_acc: 0.7875\n",
      "Epoch 207/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4649 - acc: 0.8527 - val_loss: 1.0968 - val_acc: 0.8042\n",
      "Epoch 208/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4716 - acc: 0.8542 - val_loss: 1.1569 - val_acc: 0.8000\n",
      "Epoch 209/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4658 - acc: 0.8546 - val_loss: 1.0531 - val_acc: 0.7990\n",
      "Epoch 210/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4889 - acc: 0.8542 - val_loss: 1.0671 - val_acc: 0.7865\n",
      "Epoch 211/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4743 - acc: 0.8532 - val_loss: 1.2321 - val_acc: 0.7865\n",
      "Epoch 212/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5158 - acc: 0.8523 - val_loss: 1.1048 - val_acc: 0.7969\n",
      "Epoch 213/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.5146 - acc: 0.8542 - val_loss: 1.0439 - val_acc: 0.7896\n",
      "Epoch 214/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4470 - acc: 0.8561 - val_loss: 1.1428 - val_acc: 0.7875\n",
      "Epoch 215/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4602 - acc: 0.8471 - val_loss: 1.1290 - val_acc: 0.7969\n",
      "Epoch 216/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4628 - acc: 0.8565 - val_loss: 1.0498 - val_acc: 0.7812\n",
      "Epoch 217/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4520 - acc: 0.8575 - val_loss: 1.0357 - val_acc: 0.8010\n",
      "Epoch 218/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4686 - acc: 0.8518 - val_loss: 1.2300 - val_acc: 0.7937\n",
      "Epoch 219/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4835 - acc: 0.8475 - val_loss: 1.1195 - val_acc: 0.7948\n",
      "Epoch 220/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4588 - acc: 0.8523 - val_loss: 1.0882 - val_acc: 0.7969\n",
      "Epoch 221/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4281 - acc: 0.8575 - val_loss: 1.0364 - val_acc: 0.8010\n",
      "Epoch 222/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4704 - acc: 0.8513 - val_loss: 1.1542 - val_acc: 0.7990\n",
      "Epoch 223/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4358 - acc: 0.8475 - val_loss: 1.0208 - val_acc: 0.8177\n",
      "Epoch 224/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4294 - acc: 0.8551 - val_loss: 1.0490 - val_acc: 0.8052\n",
      "Epoch 225/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4395 - acc: 0.8546 - val_loss: 1.2998 - val_acc: 0.7927\n",
      "Epoch 226/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4297 - acc: 0.8608 - val_loss: 1.0123 - val_acc: 0.8104\n",
      "Epoch 227/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4546 - acc: 0.8556 - val_loss: 1.0136 - val_acc: 0.8083\n",
      "Epoch 228/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4311 - acc: 0.8679 - val_loss: 1.3543 - val_acc: 0.7573\n",
      "Epoch 229/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4385 - acc: 0.8532 - val_loss: 1.1977 - val_acc: 0.7833\n",
      "Epoch 230/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4207 - acc: 0.8546 - val_loss: 0.9505 - val_acc: 0.8125\n",
      "Epoch 231/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4478 - acc: 0.8584 - val_loss: 1.0557 - val_acc: 0.8021\n",
      "Epoch 232/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4509 - acc: 0.8627 - val_loss: 0.9392 - val_acc: 0.8167\n",
      "Epoch 233/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4569 - acc: 0.8580 - val_loss: 0.9379 - val_acc: 0.8073\n",
      "Epoch 234/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4756 - acc: 0.8561 - val_loss: 0.9464 - val_acc: 0.7875\n",
      "Epoch 235/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4722 - acc: 0.8551 - val_loss: 0.9185 - val_acc: 0.7937\n",
      "Epoch 236/250\n",
      "2112/2112 [==============================] - 4s 2ms/step - loss: 0.4816 - acc: 0.8494 - val_loss: 1.0500 - val_acc: 0.7854\n",
      "Epoch 237/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4577 - acc: 0.8556 - val_loss: 1.0232 - val_acc: 0.8135\n",
      "Epoch 238/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4853 - acc: 0.8542 - val_loss: 1.0190 - val_acc: 0.8052\n",
      "Epoch 239/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4601 - acc: 0.8527 - val_loss: 1.0330 - val_acc: 0.8042\n",
      "Epoch 240/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4500 - acc: 0.8580 - val_loss: 1.0961 - val_acc: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4220 - acc: 0.8594 - val_loss: 1.1694 - val_acc: 0.7719\n",
      "Epoch 242/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4617 - acc: 0.8565 - val_loss: 1.0407 - val_acc: 0.7865\n",
      "Epoch 243/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4602 - acc: 0.8509 - val_loss: 1.0396 - val_acc: 0.8073\n",
      "Epoch 244/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4298 - acc: 0.8537 - val_loss: 1.0971 - val_acc: 0.7958\n",
      "Epoch 245/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4589 - acc: 0.8570 - val_loss: 1.0383 - val_acc: 0.7979\n",
      "Epoch 246/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4553 - acc: 0.8613 - val_loss: 1.1919 - val_acc: 0.7771\n",
      "Epoch 247/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4731 - acc: 0.8594 - val_loss: 1.2561 - val_acc: 0.7906\n",
      "Epoch 248/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4428 - acc: 0.8622 - val_loss: 1.0945 - val_acc: 0.8083\n",
      "Epoch 249/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4241 - acc: 0.8651 - val_loss: 1.0889 - val_acc: 0.8115\n",
      "Epoch 250/250\n",
      "2112/2112 [==============================] - 3s 2ms/step - loss: 0.4209 - acc: 0.8603 - val_loss: 1.1915 - val_acc: 0.7969\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizers.rmsprop(lr=0.0005, decay=1e-6),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"]) #lr 0.0005\n",
    "\n",
    "# Train the model\n",
    "print(\"[INFO] training model...\")\n",
    "history = model.fit(trainFeatureX.reshape(2112, 128,1), trainY, validation_data=(testFeatureX.reshape(960, 128,1), testY), epochs=250, batch_size=32)  # 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluating...\n",
      "960/960 [==============================] - 0s 125us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1914734919865926, 0.796875]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "print(\"[INFO] Evaluating...\")\n",
    "model.evaluate(testFeatureX.reshape(960, 128,1), testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras_applications' has no attribute 'set_keras_submodules'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-98cd3281928c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\applications\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_applications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m keras_applications.set_keras_submodules(\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras_applications' has no attribute 'set_keras_submodules'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
